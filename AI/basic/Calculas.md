## Chain Rule
In deep learning, the gradients of concern are often difficult to calculate because we are working with deeply nested functions (of functions (of functionsâ€¦)). Fortunately, theÂ _chain rule_Â takes care of this. Returning to functions of a single variable, suppose thatÂ ğ‘¦=ğ‘“(ğ‘”(ğ‘¥))Â and that the underlying functionsÂ ğ‘¦=ğ‘“(ğ‘¢)Â andÂ ğ‘¢=ğ‘”(ğ‘¥)Â are both differentiable. The chain rule states that
$$
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}.
$$
Turning back to multivariate functions, suppose thatÂ ğ‘¦=ğ‘“(ğ‘¢)Â has variablesÂ ğ‘¢1,ğ‘¢2,â€¦,ğ‘¢ğ‘š, where eachÂ ğ‘¢ğ‘–=ğ‘”ğ‘–(ğ‘¥)Â has variablesÂ ğ‘¥1,ğ‘¥2,â€¦,ğ‘¥ğ‘›, i.e.,Â ğ‘¢=ğ‘”(ğ‘¥). Then the chain rule states that
$$
\frac{\partial y}{\partial x_{i}} = \frac{\partial y}{\partial u_{1}} \frac{\partial u_{1}}{\partial x_{i}} + \frac{\partial y}{\partial u_{2}} \frac{\partial u_{2}}{\partial x_{i}} + \ldots + \frac{\partial y}{\partial u_{m}} \frac{\partial u_{m}}{\partial x_{i}} \ \textrm{ and so } \ \nabla_{\mathbf{x}} y =  \mathbf{A} \nabla_{\mathbf{u}} y,
$$
whereÂ ğ´âˆˆğ‘…ğ‘›Ã—ğ‘šÂ is aÂ _matrix_Â that contains the derivative of vectorÂ ğ‘¢Â with respect to vectorÂ ğ‘¥. Thus, evaluating the gradient requires computing a vectorâ€“matrix product. This is one of the key reasons why linear algebra is such an integral building block in building deep learning systems.


Simply:
